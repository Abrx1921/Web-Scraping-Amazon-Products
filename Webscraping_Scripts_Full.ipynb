{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping Scripts\n",
    "\n",
    "All of the webscraping code can be found here. \n",
    "\n",
    "- laptop_data = Title, Price, Rating, Touchscreen, Color, Ram\n",
    "- gpu_model = GPU\n",
    "- cpu_model = CPU \n",
    "- reference_laptop_lenovo = Title, Price, Rating, Touchscreen, Color, Ram (Concatenated this at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time # To add delay between requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping links from search results page: (Export: amazon_links_nosponsors.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab URL from search results page\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Headers to avoid being flagged as a bot\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    # Base URL (first page of search results)\n",
    "    base_url = 'https://www.amazon.com/s?k=gaming+laptop&rh=n%3A21512780011%2Cp_36%3A59000-81000%2Cp_n_size_browse-bin%3A2423841011&dc&crid=2DJRTPGBY27LI&qid=1733934855&rnid=2242797011&sprefix=gamin%2Caps%2C416&ref=sr_pg_1'\n",
    "\n",
    "    # List to store product links\n",
    "    links_list = []\n",
    "    current_url = base_url\n",
    "    page_count = 0  # Counter to track the number of pages scraped\n",
    "\n",
    "    while current_url and page_count < 5:  # Limit scraping to 5 pages\n",
    "        print(f\"Scraping URL: {current_url}\")\n",
    "        # Request to GET the data from the webpage\n",
    "        webpage = requests.get(current_url, headers=HEADERS)\n",
    "\n",
    "        # Check if GET request was successful\n",
    "        if webpage.status_code == 200:\n",
    "            # Parse the webpage\n",
    "            soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "\n",
    "            # Find the product links\n",
    "            links = soup.find_all('a', attrs={'class': 'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "\n",
    "            # Loop through all anchor tags and extract links\n",
    "            for link in links:\n",
    "                href = link.get('href') #get the href for all the links\n",
    "                # Skip sponsored links\n",
    "                if href and '/sspa/click' in href:  # Check for the sponsored segment in href, this will get rid of sponsors when scraping\n",
    "                    continue\n",
    "                full_link = 'https://www.amazon.com' + href # this will complete the link once href is got\n",
    "                links_list.append(full_link) # Append the link to our list where our links will be saved\n",
    "\n",
    "            # Increment page count\n",
    "            page_count += 1\n",
    "            print(f\"Scraped page {page_count}\")\n",
    "\n",
    "            # Find the next page link\n",
    "            next_page = soup.find('a', attrs={'class': 's-pagination-item s-pagination-button s-pagination-button-accessibility'})\n",
    "            if next_page and 'href' in next_page.attrs:\n",
    "                current_url = 'https://www.amazon.com' + next_page['href']\n",
    "                time.sleep(2)  # Delay to avoid being blocked\n",
    "            else:\n",
    "                current_url = None  # No more pages available\n",
    "        else:\n",
    "            print(f\"Failed to retrieve: {webpage.status_code}\")\n",
    "            break\n",
    "\n",
    "    # Save the links to a file\n",
    "    with open('amazon_links_nosponsors.txt', 'w') as file:\n",
    "        for link in links_list:\n",
    "            file.write(link + '\\n')  # Write each link on a new line\n",
    "\n",
    "    # Confirmation message\n",
    "    print(\"Links saved to 'amazon_links_nosponsors.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping links product pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscrape: Products \n",
    "\n",
    "# Grab the text file amazon_links_nosponsors.txt and run it through a script that will return the product specifications of our choosing\n",
    "def main(URL):\n",
    "\n",
    "    # Create saved file to store gathered data, itca as a .csv\n",
    "\n",
    "    with open('amazon_laptops_scraped.csv', 'a', encoding = 'utf-8') as File:\n",
    "\n",
    "        HEADERS = ({  # User-Agent mimics a real browser \n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', \n",
    "            'Accept-Language': 'en-US, en;q=0.5'\n",
    "            })               \n",
    "\n",
    "        # Specify URL\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "        # Creating the Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "        # scrape product title\n",
    "        try:\n",
    "            # Attempt to find the <span> element with an attribute id=\"productTitle\"\n",
    "            title = soup.find(\"span\", attrs={\"id\": 'productTitle'})\n",
    "            # Get the string content (text) of the found <span> element\n",
    "            title_value = title.string\n",
    "            # Remove leading/trailing whitespace and replace commas with empty strings\n",
    "            title_string = title_value.strip().replace(',', '')\n",
    "                \n",
    "        except AttributeError:\n",
    "            # If an AttributeError occurs (e.g., element not found or no string content)\n",
    "            # Set the title_string to \"None\" (converted to NaN in DataFrames)\n",
    "            title_string = None\n",
    "\n",
    "        # scrape price\n",
    "        try:\n",
    "            # Find the element containing the whole number part of the price\n",
    "            price_whole = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "            # Find the element containing the fractional part of the price\n",
    "            price_fraction = soup.find(\"span\", class_=\"a-price-fraction\")\n",
    "            if price_whole and price_fraction:\n",
    "                # If both elements are found, concatenate their text content to form the complete price\n",
    "                price = price_whole.get_text(strip=True) + price_fraction.get_text(strip=True)                \n",
    "                # Introduce a delay (e.g., to avoid hitting request limits or mimic human interaction)\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                # If either part of the price is missing, set price to None\n",
    "                price = None\n",
    "        except AttributeError:\n",
    "            # Handle cases where an AttributeError occurs (e.g., if elements are not found) by setting price to None\n",
    "            price = None\n",
    "\n",
    "        #scrape rating\n",
    "        try:\n",
    "            # Find the span using the 'data-hook' attribute\n",
    "            rating = soup.find('span', attrs={\"data-hook\": \"rating-out-of-text\"})\n",
    "            if rating:\n",
    "                # Extract the text and split to get the numerical part (e.g., \"4.4\")\n",
    "                rating_value = rating.get_text(strip=True).split(' ')[0]\n",
    "            else:\n",
    "                rating_value = None\n",
    "        except AttributeError:\n",
    "            rating_value = None\n",
    "\n",
    "        # Scape to see if there Touchscreen in the title\n",
    "        touchscreen = 'Yes' if title_string and 'touchscreen' in title_string.lower() else 'No'\n",
    "\n",
    "        # scrape color\n",
    "        # Initialize color as None to handle cases where the color cannot be found\n",
    "        color = None\n",
    "        try:\n",
    "            # Find the <th> element with specific classes and containing the word 'Color' (case-sensitive)\n",
    "            color_th = soup.find('th', class_=\"a-color-secondary a-size-base prodDetSectionEntry\", string=lambda text: text and 'Color' in text.strip())\n",
    "            if color_th:\n",
    "                # If the <th> element is found, find the next <td> element with the specified classes\n",
    "                color_td = color_th.find_next('td', class_=\"a-size-base prodDetAttrValue\")\n",
    "                if color_td:\n",
    "                    # If the <td> element is found, extract its text content, removing extra spaces\n",
    "                    color = color_td.get_text(strip=True)\n",
    "                    # Clean up the extracted color text by removing hyphens and extra spaces\n",
    "                    color = color.replace('-', '').strip()\n",
    "        except AttributeError:\n",
    "            # Handle cases where an AttributeError occurs by ensuring color remains None\n",
    "            color = None    \n",
    "\n",
    "        # Scrape Ram\n",
    "        ram = None\n",
    "        try:\n",
    "        # Locate the <tr> element by its class\n",
    "            ram_tr = soup.find('tr', class_=\"a-spacing-small po-ram_memory.installed_size\")\n",
    "            if ram_tr:\n",
    "        # Find the <td> with the class \"a-span9\" inside the row\n",
    "                ram_td = ram_tr.find('td', class_=\"a-span9\")\n",
    "                if ram_td:\n",
    "            # Extract the RAM value from the <span>\n",
    "                    ram_span = ram_td.find('span', class_=\"a-size-base po-break-word\")\n",
    "                    if ram_span:\n",
    "                        ram = ram_span.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            ram = None\n",
    "\n",
    "        # Write headers only once (if the file is empty)\n",
    "        File.seek(0, 2)  # Move to the end of the file\n",
    "        if File.tell() == 0:  # If file is empty, write the header\n",
    "            File.write(\"Title, Price, Rating, Touchscreen, Color, Ram\\n\")\n",
    "\n",
    "        # saving data to csv\n",
    "        File.write(f\"{title_string}, {price}, {rating_value}, {touchscreen}, {color}, {ram}\\n\")\n",
    "\n",
    "        # Print results for verification\n",
    "        print(f\"Title: {title_string}\")\n",
    "        print(f\"Price: {price}\")\n",
    "        print(f\"Rating: {rating_value}\")\n",
    "        print(f\"Touchscreen: {touchscreen}\")\n",
    "        print(f\"Color: {color}\")\n",
    "        print(f\"Ram: {ram}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #opening our url file to access URLs\n",
    "    with open(\"amazon_links_nosponsors.txt\", \"r\")  as file:\n",
    "    #iterating over the urls\n",
    "        for links in file.readlines():\n",
    "            main(links.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping GPU model from product pages (links):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gpu(product_url):\n",
    "\n",
    "    with open('gpu_model_data.csv', 'a', encoding='utf-8') as File:\n",
    "\n",
    "        HEADERS = ({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'\n",
    "        })\n",
    "\n",
    "        # Send an HTTP GET request to the given URL\n",
    "        webpage = requests.get(product_url, headers=HEADERS)\n",
    "\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(webpage.content, 'lxml')\n",
    "\n",
    "        gpu_model = None\n",
    "\n",
    "        try:\n",
    "            # Locate the <th> with 'Graphics Coprocessor'\n",
    "            gpu_th = soup.find('th', class_=\"a-color-secondary a-size-base prodDetSectionEntry\", \n",
    "                               string=lambda text: text and 'Graphics Coprocessor' in text.strip())\n",
    "            if gpu_th:\n",
    "                # Locate the corresponding <td> for the GPU\n",
    "                gpu_td = gpu_th.find_next('td', class_=\"a-size-base prodDetAttrValue\")\n",
    "                if gpu_td:\n",
    "                    gpu_model = gpu_td.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            gpu_model = None\n",
    "\n",
    "        # Remove the U+200E character if it exists\n",
    "        if gpu_model:\n",
    "            gpu_model = gpu_model.replace(\"\\u200E\", \"\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        print('GPU:', gpu_model)\n",
    "\n",
    "        # Save the GPU model to the file\n",
    "        File.write(f'{gpu_model}\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('amazon_links_nosponsors.txt', 'r') as file:\n",
    "        for links in file.readlines():\n",
    "            scrape_gpu(links.strip())\n",
    "\n",
    "# Explore GPU model data\n",
    "gpu = pd.read_csv('gpu_model_data.csv', names = ['gpu'])\n",
    "\n",
    "# Export as .CSV with index as column for merging\n",
    "gpu.to_csv('gpu_model.csv', index = True, index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore GPU model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore GPU model data\n",
    "gpu = pd.read_csv('gpu_model_data.csv', names = ['gpu'])\n",
    "\n",
    "# Export as .CSV with index as column for merging\n",
    "gpu.to_csv('gpu_model.csv', index = True, index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping CPU model from product pages (links):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(URL):\n",
    "    # Create saved file to store gathered data, with UTF-8 encoding\n",
    "    with open('cpu_model_data.csv', 'a', encoding='utf-8') as File:\n",
    "\n",
    "        HEADERS = ({  # User-Agent mimics a real browser \n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', \n",
    "            'Accept-Language': 'en-US, en;q=0.5'\n",
    "            })               \n",
    "\n",
    "        # Specify URL\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "        # Creating the Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "        # Try to find the 'th' with text containing 'CPU Model Number' more flexibly\n",
    "        cpu_model = None  # Default to N/A if not found\n",
    "        try:\n",
    "            cpu_header = soup.find('th', class_='a-color-secondary a-size-base prodDetSectionEntry', string=lambda text: text and 'CPU Model Number' in text.strip())\n",
    "            if cpu_header:\n",
    "                # Get the next sibling td with the class 'a-size-base prodDetAttrValue' that contains the CPU model\n",
    "                cpu_td = cpu_header.find_next('td', class_='a-size-base prodDetAttrValue')\n",
    "                if cpu_td:\n",
    "                    cpu_model = cpu_td.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            cpu_model = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        print(\"CPU:\", cpu_model)\n",
    "\n",
    "        # Saving data to CSV\n",
    "        File.write(f\"{cpu_model}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Opening our URL file to access URLs\n",
    "    with open(\"amazon_links_nosponsors.txt\", \"r\") as file:\n",
    "        # Iterating over the URLs\n",
    "        for links in file.readlines():\n",
    "            main(links.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging .CSVs for CPU model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One webscrape would not work as not all results would return. Several scrapes neccessary to get desired data.\n",
    "\n",
    "# Merge and Explore CPU model data\n",
    "\n",
    "# Load the CPU model web scrape as a dictionary to load all files at once\n",
    "CSVs = { \n",
    "    'csv1': pd.read_csv('cpu_model_data.csv', header=None, encoding='utf-8'),\n",
    "    'csv2': pd.read_csv('cpu_model_data_2.csv', header=None, encoding='utf-8'),\n",
    "    'csv3': pd.read_csv('cpu_model_data_3.csv', header=None, encoding='utf-8'),\n",
    "    'csv4': pd.read_csv('cpu_model_data_4.csv', header=None, encoding='utf-8'),\n",
    "    'csv5': pd.read_csv('cpu_model_data_5.csv', header=None, encoding='utf-8'),\n",
    "    'csv6': pd.read_csv('cpu_model_data_6.csv', header=None),\n",
    "    'csv7': pd.read_csv('cpu_model_data_7.csv', header=None), \n",
    "    'csv8': pd.read_csv('cpu_model_data_8.csv', header=None)\n",
    "}\n",
    "\n",
    "# Merge Dataframes\n",
    "cpu = pd.concat([CSVs['csv1'], CSVs['csv2'], CSVs['csv3'], CSVs['csv4'], CSVs['csv5'], CSVs['csv6'], CSVs['csv7'], CSVs['csv8']], axis=1)\n",
    "\n",
    "# Fill N/A's with cpu name using bfill ( which takes the first valid value to the right of the NaN and fills it in)\n",
    "cpu = cpu.bfill(axis=1)\n",
    "\n",
    "# Drop Columns, Keep first\n",
    "cpu = cpu.iloc[:, :1]\n",
    "\n",
    "# Rename column name\n",
    "cpu.columns = ['cpu']\n",
    "\n",
    "# Export as a .CSV file\n",
    "cpu.to_csv('cpu_model.csv', index = True, index_label=['index']) #include index as column for merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping reference computer product page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscrape: Product (Reference Laptop)\n",
    "\n",
    "def laptop(product_url):\n",
    "\n",
    "    # Create saved file to store gathered data\n",
    "\n",
    "    with open('reference_laptop.csv', 'a', encoding = 'utf-8') as File:\n",
    "\n",
    "        HEADERS = ({  # User-Agent mimics a real browser \n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', \n",
    "            'Accept-Language': 'en-US, en;q=0.5'\n",
    "            })               \n",
    "\n",
    "        # Specify URL\n",
    "        webpage = requests.get(product_url, headers=HEADERS)\n",
    "\n",
    "        # Creating the Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "        # scrape title\n",
    "        try:\n",
    "            # Attempt to find the <span> element with an attribute id=\"productTitle\"\n",
    "            title = soup.find(\"span\", attrs={\"id\": 'productTitle'})\n",
    "            # Get the string content (text) of the found <span> element\n",
    "            title_value = title.string\n",
    "            # Remove leading/trailing whitespace and replace commas with empty strings\n",
    "            title_string = title_value.strip().replace(',', '')\n",
    "                \n",
    "        except AttributeError:\n",
    "            # If an AttributeError occurs (e.g., element not found or no string content)\n",
    "            # Set the title_string to \"NA\" (Not Available)\n",
    "            title_string = None\n",
    "            # Print a message indicating the product title is unavailable\n",
    "        \n",
    "\n",
    "        # scrape price\n",
    "\n",
    "        try:\n",
    "            price_whole = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "            price_fraction = soup.find(\"span\", class_=\"a-price-fraction\")\n",
    "\n",
    "            if price_whole and price_fraction:\n",
    "                price = price_whole.get_text(strip = True) + price_fraction.get_text(strip = True)\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                price = None\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "        \n",
    "\n",
    "        #scrape rating\n",
    "        try:\n",
    "            # Find the span using the 'data-hook' attribute\n",
    "            rating = soup.find('span', attrs={\"data-hook\": \"rating-out-of-text\"})\n",
    "            if rating:\n",
    "                # Extract the text and split to get the numerical part (e.g., \"4.4\")\n",
    "                rating_value = rating.get_text(strip=True).split(' ')[0]\n",
    "            else:\n",
    "                rating_value = None\n",
    "        except AttributeError:\n",
    "            rating_value = None\n",
    "\n",
    "\n",
    "        # Scape to see if there Touchscreen in the title\n",
    "\n",
    "        touchscreen = 'Yes' if title_string and 'touchscreen' and 'touch' in title_string.lower() else 'No'\n",
    "\n",
    "\n",
    "        # scrape color\n",
    "        color = None\n",
    "        try:\n",
    "            color_th = soup.find('th', class_=\"a-color-secondary a-size-base prodDetSectionEntry\", string = lambda text: text and 'Color' in text.strip())\n",
    "            if color_th:\n",
    "                color_td = color_th.find_next('td', class_=\"a-size-base prodDetAttrValue\")\n",
    "                if color_td:\n",
    "                    color = color_td.get_text(strip = True)\n",
    "                    color = color.replace('-', '').strip()\n",
    "        except AttributeError:\n",
    "            color = None\n",
    "\n",
    "\n",
    "        # Scrape Ram\n",
    "        ram = None\n",
    "        try:\n",
    "            # Locate the <tr> element by its class\n",
    "            ram_tr = soup.find('tr', class_=\"a-spacing-small po-ram_memory.installed_size\")\n",
    "            if ram_tr:\n",
    "            # Find the <td> with the class \"a-span9\" inside the row\n",
    "                ram_td = ram_tr.find('td', class_=\"a-span9\")\n",
    "                if ram_td:\n",
    "                    # Extract the RAM value from the <span>\n",
    "                    ram_span = ram_td.find('span', class_=\"a-size-base po-break-word\")\n",
    "                    if ram_span:\n",
    "                        ram = ram_span.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            ram = None\n",
    "\n",
    "\n",
    "        #Scrape Gpu\n",
    "        try:\n",
    "            # Locate the <th> with 'Graphics Coprocessor'\n",
    "            gpu_th = soup.find('th', class_=\"a-color-secondary a-size-base prodDetSectionEntry\", \n",
    "                               string=lambda text: text and 'Graphics Coprocessor' in text.strip())\n",
    "            if gpu_th:\n",
    "                # Locate the corresponding <td> for the GPU\n",
    "                gpu_td = gpu_th.find_next('td', class_=\"a-size-base prodDetAttrValue\")\n",
    "                if gpu_td:\n",
    "                    gpu = gpu_td.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            gpu = None\n",
    "\n",
    "\n",
    "        # Remove the U+200E character if it exists\n",
    "        if gpu:\n",
    "            gpu = gpu.replace(\"\\u200E\", \"\")\n",
    "\n",
    "\n",
    "        # Try to find the 'th' with text containing 'CPU Model Number' more flexibly\n",
    "        cpu = None  # Default to N/A if not found\n",
    "        try:\n",
    "            cpu_header = soup.find('th', class_='a-color-secondary a-size-base prodDetSectionEntry', string=lambda text: text and 'CPU Model Number' in text.strip())\n",
    "            if cpu_header:\n",
    "                # Get the next sibling td with the class 'a-size-base prodDetAttrValue' that contains the CPU model\n",
    "                cpu_td = cpu_header.find_next('td', class_='a-size-base prodDetAttrValue')\n",
    "                if cpu_td:\n",
    "                    cpu = cpu_td.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            cpu = None\n",
    "\n",
    "        # Write headers only once (if the file is empty)\n",
    "        File.seek(0, 2)  # Move to the end of the file\n",
    "        if File.tell() == 0:  # If file is empty, write the header\n",
    "            File.write(\"title,price,rating,touchscreen,color,ram,gpu,cpu\\n\")\n",
    "\n",
    "        # saving data to csv\n",
    "        File.write(f\"{title_string},{price},{rating_value},{touchscreen},{color},{ram},{gpu},{cpu} \\n\")\n",
    "\n",
    "        # Print results for verification\n",
    "        print(f\"Title: {title_string}\")\n",
    "        print(f\"Price: {price}\")\n",
    "        print(f\"Rating: {rating_value}\")\n",
    "        print(f\"Touchscreen: {touchscreen}\")\n",
    "        print(f\"Color: {color}\")\n",
    "        print(f\"Ram: {ram}\")\n",
    "        print(f\"Gpu: {gpu}\")\n",
    "        print(f\"Cpu: {cpu}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #opening our url file to access URLs\n",
    "    with open(\"reference_laptop_link.txt\", \"r\")  as file:\n",
    "    #iterating over the urls\n",
    "        for links in file.readlines():\n",
    "            laptop(links.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean before merging: (reference laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean before merging\n",
    "\n",
    "# Convert to a dataframe\n",
    "reference = pd.read_csv('reference_laptop.csv', index_col=None)\n",
    "\n",
    "# Ensure consistent column names\n",
    "reference.columns = reference.columns.str.lower()\n",
    "\n",
    "# Add link column to df\n",
    "link = ['https://www.amazon.com/Lenovo-i7-12700H-Fingerprint-Long-Lasting-Charging/dp/B0DHKCQPN7/ref=sr_1_10?crid=1XTREJPIHYSUN&dib=eyJ2IjoiMSJ9.8migpGwkWg9b9GXp-dZFZZL8PHkty7Gd6hNXqkkapByJBFg0c3lAeKxOaQ5xJABrIYalWaht9S7Lnv5BycgACMOfN0FYb0AAcISSsU9KcG1biTUFpkEAgQ3BKbhPJxVRph5yaKHEteA6jbK6i3SBPHxcP2X_NQ55zqJRvIvo09o4sf9TD0ZkRbXFV4diCECz4gJaHTFfPB1jDW2rJCoAXkRQWK7XiQQtFUiKOv63Wft07b4WN_QdiJA-Nrrvh7W55bA5jvsurjfn9Jm-MlkBgfrYO1fxORpyKpFIfiCVcrqbU1SzoOIcOykVBieB19lqgBxGHXVV5wzq-CwgFZqQQLPIBurJCJMmblfamjJxCJQ.AEKbSTRrUtoORhQqbVCGDp2JsNyGbCV8DKs6e8_7dI0&dib_tag=se&keywords=Lenovo%2B-%2BYoga%2B7%2B2-in-1%2B14%22%2B2K%2BTouchscreen%2BLaptop%2B-%2BAMD%2BRyzen%2B7%2B8840HS%2Bwith%2B16GB%2BMemory%2B-%2B1%2BTB%2BSSD%2B-%2BArtic%2BGrey&qid=1734398677&s=electronics&sprefix=lenovo%2B-%2Byoga%2B7%2B2-in-1%2B14%2B2k%2Btouchscreen%2Blaptop%2B-%2Bamd%2Bryzen%2B7%2B8840hs%2Bwith%2B16gb%2Bmemory%2B-%2B1%2Btb%2Bssd%2B-%2Bartic%2Bgrey%2Celectronics%2C179&sr=1-10&th=1']\n",
    "\n",
    "reference['link'] = link\n",
    "\n",
    "# Convert the reference link column to HTML anchor tags (clickable links)\n",
    "reference['link'] = reference['link'].apply(lambda x: f'<a href=\"{x}\" target=\"_blank\">{x}</a>')\n",
    "\n",
    "# Export as .csv\n",
    "reference.to_csv('reference_laptop_lenovo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "cpu_model = pd.read_csv('cpu_model.csv')\n",
    "gpu_model = pd.read_csv('gpu_model.csv')\n",
    "laptop_data = pd.read_csv('laptop_data.csv')\n",
    "\n",
    "# Data exploration: pre-merge\n",
    "laptop_data.head()\n",
    "cpu_model.head()\n",
    "gpu_model.head()\n",
    "\n",
    "cpu_model.info()\n",
    "gpu_model.info()\n",
    "laptop_data.info()\n",
    "\n",
    "# Begin merging the dataset\n",
    "product_data = laptop_data.merge(cpu_model, on = 'index', how = 'left').merge(gpu_model, on = 'index', how = 'left')\n",
    "\n",
    "# Explore merged data\n",
    "product_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding column ‘links’ to dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add links to dataframe. (amazon_links_nosponsor.txt)\n",
    "links_path = r'C:\\Users\\abrah\\OneDrive\\Documents\\Data Projects\\Web Scraping Amazon Products\\amazon_links_nosponsors.txt'\n",
    "\n",
    "# Open .txt file\n",
    "with open(links_path, 'r') as file:\n",
    "    links = file.readlines()\n",
    "\n",
    "# Stripping each element in the list\n",
    "links = [link.strip() for link in links]\n",
    "\n",
    "# Create df with column name\n",
    "links_data = pd.DataFrame(links, columns = ['link'])\n",
    "\n",
    "# Adds index as a column\n",
    "links_data = links_data.reset_index()\n",
    "\n",
    "# Export to .csv\n",
    "links_data.to_csv('links_data.csv', index = False)\n",
    "\n",
    "# Explore df\n",
    "links_data\n",
    "\n",
    "# Convert the 'product_link' column to HTML anchor tags (clickable links)\n",
    "links_data = links_data['link'].apply(lambda x: f'<a href=\"{x}\" target=\"_blank\">{x}</a>').reset_index()\n",
    "\n",
    "# merge link_data with dataframe\n",
    "dataframe_final = product_data.merge(links_data, on = 'index', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the newly merged dataframe and export final as .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy\n",
    "df = dataframe_final.copy()\n",
    "\n",
    "# Look for duplicates specific to the title column (due to us merging links, all links are different although product title might be the same)\n",
    "duplicates = df[df.duplicated(subset='title', keep=False)] # keep=False: Marks all occurrences of duplicates as True.\n",
    "\n",
    "# View duplicates\n",
    "duplicates.duplicated().value_counts() # 48 total duplicates\n",
    "\n",
    "# Remove duplicates ('title' column) and keep only the first occurrence, route back to df (copy)\n",
    "df = df.drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "#Look at dtypes and columns \n",
    "df.info()\n",
    "\n",
    "# remove whitespaces from column names\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "# Delete index column and Unamed: 0 column\n",
    "del df['index']\n",
    "del laptop_data['Unnamed: 0']\n",
    "\n",
    "# Clean 'color' column special characters [U200E] (We did this for the gpu_model too but we caught it when scrapping)\n",
    "df.loc[:, 'color'] = df['color'].str.replace('\\u200e', '', regex=True)\n",
    "\n",
    "# This will allow us to load the df with clickable links!!!\n",
    "# CSS style to reduce the font size of the links\n",
    "style = \"\"\"\n",
    "    <style>\n",
    "        a {\n",
    "            font-size: .5px; /* You can adjust this size */\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "# Display the DataFrame with clickable links\n",
    "from IPython.display import HTML\n",
    "HTML(style + df.to_html(escape=False))\n",
    "\n",
    "# Add the laptop we are referencing\n",
    "reference_laptop = pd.read_csv('reference_laptop_lenovo.csv')\n",
    "# Concat with dataframe\n",
    "df = pd.concat([df, reference_laptop], axis = 0, ignore_index=True)\n",
    "# Delete column (loaded again because of concat)\n",
    "del df['Unnamed: 0'] \n",
    "\n",
    "# Convert proper datatypes \n",
    "# Strip any leading/trailing spaces and then convert to numeric, replacing invalid values with NaN\n",
    "df['rating'] = pd.to_numeric(df['rating'].str.strip(), errors='coerce')\n",
    "# Convert rating to float dtype\n",
    "df['rating'] = df['rating'].astype('float')\n",
    "\n",
    "# Export to .csv (no index)\n",
    "df.to_csv('final_laptop_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "We successfully created a complete dataset named final_laptop_data.csv, containing web-scraped data from Amazon to help my cousin find the perfect laptop. The dataset includes 33 rows and 9 columns, capturing the following details:\n",
    "\n",
    "Columns: Title, Price, Rating, Touchscreen, Color, Ram, GPU, CPU and Link\n",
    "\n",
    "Purpose:\n",
    "The goal of this project was to compare laptops similar to a specific model my cousin liked, while exploring other options within her price range that might offer better value or features.\n",
    "\n",
    "Source:\n",
    "All laptops were sourced from Amazon's listings through web scraping.\n",
    "\n",
    "Considerations:\n",
    "While the data is comprehensive, it may require cleaning during the analysis phase to address any inconsistencies or missing values. Some limitations exist and can be addressed with further cleaning or webscraping with the scripts provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
